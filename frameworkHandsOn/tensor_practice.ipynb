{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit",
   "display_name": "Python 3.7.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "b29f500530640cf719157a4700b88c233ad6291ecd37b6844873f03a361a51ac"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Basic Tensors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "data = np.array([1, 2, 3])\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "torch.Tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "torch.tensor(data)  #factory function that matches the input data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "torch.as_tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "torch.from_numpy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [0., 1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "torch.eye(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "torch.zeros(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "torch.ones(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.2411, 0.8403],\n",
       "        [0.2613, 0.4313]])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "torch.rand(2,2)"
   ]
  },
  {
   "source": [
    "# Creating PyTorch Tensors -- Best Options"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([1, 2, 3])"
   ]
  },
  {
   "source": [
    "Note that apart from the Tensor class constructor invocation, the rest are all factory methods."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.Tensor(data)\n",
    "t2 = torch.tensor(data)\n",
    "t3 = torch.as_tensor(data)\n",
    "t4 = torch.from_numpy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.float32\ntorch.int64\ntorch.int64\ntorch.int64\n"
     ]
    }
   ],
   "source": [
    "print(t1.dtype)\n",
    "print(t2.dtype)\n",
    "print(t3.dtype)\n",
    "print(t4.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "torch.get_default_dtype()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.], dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "torch.tensor(np.array([1, 2, 3]), dtype=torch.float64)"
   ]
  },
  {
   "source": [
    "## Data Copying and Sharing in Tensors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.Tensor(data)\n",
    "t2 = torch.tensor(data)\n",
    "t3 = torch.as_tensor(data)\n",
    "t4 = torch.from_numpy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we modify the original numpy array\n",
    "data[0] = 0\n",
    "data[1] = 0\n",
    "data[2] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([1., 2., 3.])\ntensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(t1)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(t4)"
   ]
  },
  {
   "source": [
    "The tensors `t3` and `t4` are also modified! It turns out that `torch.Tensor` and `torch.tensor` __copy__ new data (i.e. creates new object in memory). On the other hand, `as_tensor` and `from_numpy` __share__ memory from data. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "`tensor.Tensor`    \n",
    " * copy\n",
    " * uses global data type\n",
    "\n",
    "__`tensor.tensor`__†    <----- Preferred\n",
    "* copy\n",
    "* dynamic; infers data type\n",
    "\n",
    "\n",
    "__`tensor.as_tensor`__†     <----- Preferred\n",
    "* shares memory\n",
    "* Accepts _any_ array-like object as input.\n",
    "\n",
    "`tensor.from_numpy` \n",
    "* shares memory\n",
    "* Accepts only NumPy arrays."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Flatten, Reshape and Squeeze\n",
    "\n",
    "We can categorize high-level tensor operations into four categories:\n",
    "\n",
    "- Reshaping operations\n",
    "- Element-wise operations\n",
    "- Reduction operations\n",
    "- Access operations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([\n",
    "    [1,1,1,1],\n",
    "    [2,2,2,2],\n",
    "    [3,3,3,3]\n",
    "], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "t.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "len(t.shape)"
   ]
  },
  {
   "source": [
    "To get the number of scalar components of the tensor, we perform the following operation:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(12)"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "torch.tensor(t.shape).prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "t.numel()  # Short for Number of Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [2., 2.],\n",
       "        [2., 2.],\n",
       "        [3., 3.],\n",
       "        [3., 3.]])"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "t.reshape(6, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [3.],\n",
       "        [3.],\n",
       "        [3.]])"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "t.reshape(12, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "t.reshape(1, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.],\n",
       "         [1., 2., 2.]],\n",
       "\n",
       "        [[2., 2., 3.],\n",
       "         [3., 3., 3.]]])"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "t.reshape(2, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "t.reshape(-1)      # -1 says that reshape method will figure out the value based on attributes of t"
   ]
  },
  {
   "source": [
    "## Squeeze\n",
    "\n",
    "`torch.squeeze(input, dim=None, *, out=None) → Tensor`\n",
    "\n",
    "Returns a tensor with all the dimensions of input of size 1 removed.\n",
    "\n",
    "For example, if input is of shape: `A×1×B×C×1×D` then the out tensor will be of shape: `A×B×C×D`.\n",
    "\n",
    "When dim is given, a squeeze operation is done only in the given dimension. \n",
    "\n",
    "If input is of shape: `A×1×B` , squeeze(input, 0) leaves the tensor unchanged, but squeeze(input, 1) will squeeze the tensor to the shape `A×B` .\n",
    "\n",
    "\n",
    "```\n",
    ">>> x = torch.zeros(2, 1, 2, 1, 2)\n",
    ">>> x.size()\n",
    "torch.Size([2, 1, 2, 1, 2])\n",
    ">>> y = torch.squeeze(x)\n",
    ">>> y.size()\n",
    "torch.Size([2, 2, 2])\n",
    ">>> y = torch.squeeze(x, 0)\n",
    ">>> y.size()\n",
    "torch.Size([2, 1, 2, 1, 2])\n",
    ">>> y = torch.squeeze(x, 1)\n",
    ">>> y.size()\n",
    "torch.Size([2, 2, 1, 2])\n",
    "```\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "t.reshape(1, 12)    # Note: Double brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "t.reshape(1, 12).squeeze()"
   ]
  },
  {
   "source": [
    "`torch.unsqueeze(input, dim) → Tensor`\n",
    "\n",
    "Returns a new tensor with a dimension of size one inserted at the specified position. The returned tensor shares the same underlying data with this tensor.\n",
    "\n",
    "A `dim` value within the range `[-input.dim() - 1, input.dim() + 1)` can be used. \n",
    "\n",
    "Negative dim will correspond to unsqueeze() applied at `dim = dim + input.dim() + 1`.\n",
    "\n",
    "```\n",
    ">>> x = torch.tensor([1, 2, 3, 4])\n",
    ">>> torch.unsqueeze(x, 0)\n",
    "tensor([[ 1,  2,  3,  4]])\n",
    ">>> torch.unsqueeze(x, 1)\n",
    "tensor([[ 1],\n",
    "        [ 2],\n",
    "        [ 3],\n",
    "        [ 4]])\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-35-ef893e95223c>, line 1)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-35-ef893e95223c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    t.reshape(1, 12).squeeze().unsqueeze(dim = 0).  # (1, 12) -> 12 -> (1, 12)\u001b[0m\n\u001b[0m                                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "t.reshape(1, 12).squeeze().unsqueeze(dim = 0).  # (1, 12) -> 12 -> (1, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [3.],\n",
       "        [3.],\n",
       "        [3.]])"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "t.reshape(1, 12).squeeze().unsqueeze(dim = 1)   # (1, 12) -> (12, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.]],\n",
       "\n",
       "        [[2.],\n",
       "         [2.],\n",
       "         [2.],\n",
       "         [2.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]]])"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "t.unsqueeze(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3, 4])\ntorch.Size([3, 4, 1])\ntorch.Size([3, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "print(t.shape)\n",
    "print(t.unsqueeze(dim=2).shape)\n",
    "print(t.unsqueeze(dim=1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.]],\n",
       "\n",
       "        [[2., 2., 2., 2.]],\n",
       "\n",
       "        [[3., 3., 3., 3.]]])"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "t.unsqueeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2.],\n",
       "        [3., 3., 3., 3.]])"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "source": [
    "## Flatten\n",
    "\n",
    "`torch.flatten(input, start_dim=0, end_dim=-1) → Tensor`\n",
    "\n",
    "Flattens a contiguous range of dims in a tensor.\n",
    "\n",
    "```\n",
    ">>> t = torch.tensor([[[1, 2],\n",
    "                       [3, 4]],\n",
    "                      [[5, 6],\n",
    "                       [7, 8]]])\n",
    ">>> torch.flatten(t)\n",
    "tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "\n",
    ">>> torch.flatten(t, start_dim=1)\n",
    "tensor([[1, 2, 3, 4],\n",
    "        [5, 6, 7, 8]])\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(t):\n",
    "    t= t.reshape(1, -1)\n",
    "    t = t.squeeze()\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "flatten(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "t.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "t.flatten()"
   ]
  },
  {
   "source": [
    "## Concat"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "t1 = torch.tensor([1, 2])\n",
    "t2 = torch.tensor([3, 4])\n",
    "\n",
    "torch.cat((t1, t2), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = t1.unsqueeze(dim = 0)\n",
    "t2 = t2.unsqueeze(dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "torch.cat((t1, t2), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4]])"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "torch.cat((t1, t2), dim=1)"
   ]
  },
  {
   "source": [
    "### Example: Batch image input for CNN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.ones(4, 4)\n",
    "t2 = torch.ones(4, 4) * 2 \n",
    "t3 = torch.ones(4, 4) * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]])\ntensor([[2., 2., 2., 2.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.]])\ntensor([[3., 3., 3., 3.],\n        [3., 3., 3., 3.],\n        [3., 3., 3., 3.],\n        [3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.stack((t1, t2, t3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2.]],\n",
       "\n",
       "        [[3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3.]]])"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "batch   # Rank 3 tensor that contains 3 4x4 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.]]],\n",
       "\n",
       "\n",
       "        [[[2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.]]],\n",
       "\n",
       "\n",
       "        [[[3., 3., 3., 3.],\n",
       "          [3., 3., 3., 3.],\n",
       "          [3., 3., 3., 3.],\n",
       "          [3., 3., 3., 3.]]]])"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "batch = batch.reshape(3, 1, 4, 4)   # Batch, Channel, Height, Width\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "First Image : \n tensor([[[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]]])\nFirst Color Channel : \n tensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]])\nFirst row of pixels in the first color channels: \n tensor([1., 1., 1., 1.])\nFirst pixel value in the first row of the first color channel of the first image: \n tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Lets check out the tensor via some indexing.\n",
    "print(\"First Image : \\n\" , batch[0])\n",
    "print(\"First Color Channel : \\n\", batch[0][0])\n",
    "print(\"First row of pixels in the first color channels: \\n\", batch[0][0][0])\n",
    "print(\"First pixel value in the first row of the first color channel of the first image: \\n\", batch[0][0][0][0])"
   ]
  },
  {
   "source": [
    "#### Now we fill flatten the image across each channel.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([3, 16])"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "batch.flatten(start_dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
       "        [3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.]])"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "batch.flatten(start_dim=1)  # start_dim tells us which axis to start with, in order to flatten."
   ]
  },
  {
   "source": [
    "## Element-wise Operation\n",
    "\n",
    "An element-wise Operation is an operation between two tensors that operates on corresponding elements within the respective tensors. The correspondence is determinced by indices.\n",
    "\n",
    "Also the tensors need to be of the same shape."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([\n",
    "    [1, 2],\n",
    "    [3, 4]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "t2 = torch.tensor([\n",
    "    [9, 8],\n",
    "    [7, 6]\n",
    "], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1., 2.])"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "t1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "t1[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[10., 10.],\n",
       "        [10., 10.]])"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "t1 + t2"
   ]
  },
  {
   "source": [
    "#### But in the case of a lower rank tensor, **Broadcasting** happens.\n",
    "\n",
    "Broadcasting is the concept whose implementation allows us to add scalars to higher dimensional tensors.\n",
    "\n",
    "We can see what the broadcasted scalar value looks like using the `broadcast_to()` Numpy function:\n",
    "```\n",
    "np.broadcast_to(2, t1.shape)\n",
    "array([[2, 2],\n",
    "        [2, 2]])\n",
    "```\n",
    "\n",
    "This means the scalar value is transformed into a rank-2 tensor just like t1, and just like that, the shapes match and the element-wise rule of having the same shape is back in play. This is all under the hood of course."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Even though these two tenors have differing shapes, the element-wise operation is possible, and broadcasting is what makes the operation possible. The lower rank tensor t2 will be transformed via broadcasting to match the shape of the higher rank tensor t1, and the element-wise operation will be performed as usual.\n",
    "\n",
    "The concept of broadcasting is the key to understanding how this operation will be carried out. As before, we can check the broadcast transformation using the broadcast_to() numpy function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([\n",
    "    [1, 1],\n",
    "    [1, 1]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "t2 = torch.tensor([2, 4], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[3., 5.],\n",
       "        [3., 5.]])"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "t1 + t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = torch.tensor([\n",
    "    [0, 5, 7],\n",
    "    [6, 0, 7],\n",
    "    [0, 8, 0]\n",
    "], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ True, False, False],\n",
       "        [False,  True, False],\n",
       "        [ True, False,  True]])"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "t3.eq(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True],\n",
       "        [ True, False,  True],\n",
       "        [False,  True, False]])"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "t3.ge(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[False, False,  True],\n",
       "        [ True, False,  True],\n",
       "        [False,  True, False]])"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "t3.gt(5)"
   ]
  },
  {
   "source": [
    "### Element-wise operations using functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0., 5., 7.],\n",
       "        [6., 0., 7.],\n",
       "        [0., 8., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "t3.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.0000, 2.2361, 2.6458],\n",
       "        [2.4495, 0.0000, 2.6458],\n",
       "        [0.0000, 2.8284, 0.0000]])"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "t3.sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-0., -5., -7.],\n",
       "        [-6., -0., -7.],\n",
       "        [-0., -8., -0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "t3.neg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0., 5., 7.],\n",
       "        [6., 0., 7.],\n",
       "        [0., 8., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "t3.neg().abs()"
   ]
  },
  {
   "source": [
    "## Reduction operations\n",
    "\n",
    "A reduction operation on a tensor is an operation that reduces the number of elements contained within the tensor. Tensors give us the ability to manage our data.\n",
    "\n",
    "- Reshaping operations gave us the ability to position our elements along particular axes. \n",
    "- Element-wise operations allow us to perform operations on elements between two tensors. \n",
    "- Reduction operations allow us to perform operations on elements within a single tensor."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([\n",
    "    [0, 1, 0],\n",
    "    [2, 0, 2],\n",
    "    [0, 3, 0]\n",
    "], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(8.)"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "t.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "t.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "t.sum().numel()"
   ]
  },
  {
   "source": [
    "Checking the number of elements in the original tensor against the result of the sum() call, we can see that, indeed, the tensor returned by the call to sum() contains fewer elements than the original.\n",
    "\n",
    "Since the number of elements have been reduced by the operation, we can conclude that the sum() method is a reduction operation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "t.prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.8889)"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "t.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(1.1667)"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "t.std()"
   ]
  },
  {
   "source": [
    "### Do reduction operations always reduce to a tensor with a single element?\n",
    "\n",
    "The answer is no!\n",
    "\n",
    "In fact, we often reduce specific axes at a time. This process is important. It’s just like we saw with reshaping when we aimed to flatten the image tensors within a batch while still maintaining the batch axis."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([\n",
    "    [1,1,1,1],\n",
    "    [2,2,2,2],\n",
    "    [3,3,3,3]\n",
    "], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([6., 6., 6., 6.])"
      ]
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "t.sum(dim=0)"
   ]
  },
  {
   "source": [
    "Surprise! Element-wise operations are in play here.\n",
    "\n",
    "When we sum across the first axis, we are taking the summation of all the elements of the first axis. To do this, we must utilize element-wise addition. \n",
    "\n",
    "```\n",
    "​> t[0]\n",
    "tensor([1., 1., 1., 1.])\n",
    "\n",
    "> t[1]\n",
    "tensor([2., 2., 2., 2.])\n",
    "\n",
    "> t[2]\n",
    "tensor([3., 3., 3., 3.])\n",
    "\n",
    "> t[0] + t[1] + t[2]\n",
    "tensor([6., 6., 6., 6.])\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([ 4.,  8., 12.])"
      ]
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "t.sum(dim=1)"
   ]
  },
  {
   "source": [
    "The second axis in this tensor contains numbers that come in groups of four. Since we have three groups of four numbers, we get three sums.\n",
    "\n",
    "```\n",
    "> t[0].sum()\n",
    "tensor(4.)\n",
    "\n",
    "> t[1].sum()\n",
    "tensor(8.)\n",
    "\n",
    "> t[2].sum()\n",
    "tensor(12.)\n",
    "\n",
    "> t.sum(dim=1)\n",
    "tensor([ 4.,  8., 12.])\n",
    "```\n",
    "\n",
    "The specification of `dim=k` can be thought of as all elements which differ only on the `k` axis are aggregated. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Argmax Tensor Reduction Operation\n",
    "\n",
    "Argmax returns the index location of the maximum value inside a tensor."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([\n",
    "    [1,0,0,2],\n",
    "    [0,3,3,0],\n",
    "    [4,0,0,5]\n",
    "], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "t.max()"
   ]
  },
  {
   "source": [
    "The first piece of code confirms for us that the max is indeed 5, but the call to the argmax() method tells us that the 5 is sitting at index 11. What’s happening here?\n",
    "\n",
    "We’ll have a look at the flattened output for this tensor. If we don’t specific an axis to the argmax() method, it returns the index location of the max value from the flattened tensor, which in this case is indeed 11."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(11)"
      ]
     },
     "metadata": {},
     "execution_count": 84
    }
   ],
   "source": [
    "t.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([4., 3., 3., 5.]),\n",
       "indices=tensor([2, 1, 1, 2]))"
      ]
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "source": [
    "t.max(dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([2., 3., 5.]),\n",
       "indices=tensor([3, 2, 3]))"
      ]
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "source": [
    "t.max(dim = 1)"
   ]
  },
  {
   "source": [
    "For the first axis, the max values are, 4, 3, 3, and 5. These values are determined by taking the element-wise maximum across each array running across the first axis.\n",
    "\n",
    "For each of these maximum values, the argmax() method tells us which element along the first axis where the value lives.\n",
    "\n",
    "The 4 lives at index two of the first axis.\n",
    "The first 3 lives at index one of the first axis.\n",
    "The second 3 lives at index one of the first axis.\n",
    "The 5 lives at index two of the first axis.\n",
    "For the second axis, the max values are 2, 3, and 5. These values are determined by taking the maximum inside each array of the first axis. We have three groups of four, which gives us 3 maximum values.\n",
    "\n",
    "The argmax values here, tell the index inside each respective array where the max value lives.\n",
    "\n",
    "In practice, we often use the argmax() function on a network’s output prediction tensor, to determine which category has the highest prediction value."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Accessing elements inside Tensors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([\n",
    "    [1,2,3],\n",
    "    [4,5,6],\n",
    "    [7,8,9]\n",
    "], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "source": [
    "t.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "source": [
    "t.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[4.0, 5.0, 6.0]"
      ]
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "source": [
    "t.mean(dim=0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([4., 5., 6.], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "source": [
    "t.mean(dim=0).numpy()"
   ]
  },
  {
   "source": [
    "When we compute the mean across the first axis, multiple values are returned, and we can access the numeric values by transforming the output tensor into a Python list or a NumPy array."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}