{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Build PyTorch CNN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "__ML Pipeline__: Prepare data -> __build model__ -> train model -> analyze model's results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "To build neural networks in PyTorch, we extend the `torch.nn.Module` PyTorch class. This means we need to utilize a little bit of object oriented programming (OOP) in Python."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## OOP Review\n",
    "\n",
    "When we’re writing programs or building software, there are two key components, code and data. With object oriented programming, we orient our program design and structure around objects.\n",
    "\n",
    "Objects are defined in code using classes. A class defines the object's specification or spec, which specifies what data and code each object of the class should have.\n",
    "\n",
    "When we create an object of a class, we call the object an instance of the class, and all instances of a given class have two core components:\n",
    "- Methods (code)\n",
    "- Attributes (data)\n",
    "\n",
    "The methods represent the code, while the attributes represent the data, and so the methods and attributes are defined by the class.\n",
    "\n",
    "In a given program, many objects, a.k.a instances of a given class, can exist simultaneously, and all of the instances will have the same available attributes and the same available methods. They are uniform from this perspective.\n",
    "\n",
    "The difference between objects of the same class is the values contained within the object for each attribute. Each object has its own attribute values. These values determine the internal state of the object. The code and data of each object is said to be encapsulated within the object."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lizard:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def set_name(self, name):\n",
    "        self.name = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Deer\n"
     ]
    }
   ],
   "source": [
    "lizard = Lizard('Deer')\n",
    "print(lizard.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DL\n"
     ]
    }
   ],
   "source": [
    "lizard.set_name('DL')\n",
    "print(lizard.name)"
   ]
  },
  {
   "source": [
    "## `torch.nn`\n",
    "\n",
    "As we know, deep neural networks are built using multiple layers. This is what makes the network deep. Each layer in a neural network has two primary components:\n",
    "\n",
    "* A transformation (code)\n",
    "* A collection of weights (data)\n",
    "\n",
    "Within the `nn` package, there is a class called `Module`, and it is the __base class__ for all of neural network modules which includes layers.\n",
    "\n",
    "This means that all of the layers in PyTorch extend the `nn.Module` class and inherit all of PyTorch’s built-in functionality within the `nn.Module` class. \n",
    "\n",
    "In OOP this concept is known as __inheritance__."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "source": [
    "### `forward()` method\n",
    "\n",
    "When we pass a tensor to our network as input, the __tensor flows__ forward though each layer transformation until the tensor reaches the output layer. This process of a tensor flowing forward though the network is known as a __forward pass__.\n",
    "\n",
    "Each layer has its own transformation (code) and the tensor passes forward through each layer. The composition of all the individual layer forward passes defines the overall forward pass transformation for the network. The goal of the overall transformation is to transform or map the input to the correct prediction output class, and during the training process, the layer weights (data) are updated in such a way that cause the mapping to adjust to make the output closer to the correct prediction. This is achieved efficiently by __backpropagation__.\n",
    "\n",
    "What this all means is that, every PyTorch `nn.Module` has a `forward()` method, and so when we are building layers and networks, we must provide an implementation of the `forward()` method. The forward method is the actual transformation."
   ],
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "source": [
    "### `torch.nn.functional`\n",
    "\n",
    "When we implement the `forward()` method of our `nn.Module` subclass, we will typically use functions from the `nn.functional` package. This package provides us with many neural network operations that we can use for building layers. In fact, many of the `nn.Module` layer classes use `nn.functional` functions to perform their operations.\n",
    "\n",
    "The `nn.functional` package contains methods that __subclasses__ of `nn.Module` use for implementing their `forward()` functions. One reason for this is that during backpropagation, the network must perform a __symbolic differentiation__ of the operations involved in the layers to calculate the gradient of the loss with respect to the weights."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Building a Neural Network in PyTorch\n",
    "\n",
    "We now have enough information to provide an outline for building neural networks in PyTorch. The steps are as follows:\n",
    "\n",
    "Short version:\n",
    "\n",
    "- Extend the `nn.Module` base class.\n",
    "- Define layers as class attributes.\n",
    "- Implement the `forward()` method.\n",
    "\n",
    "\n",
    "More detailed version:\n",
    "\n",
    "- Create a neural network class that extends the `nn.Module` base class.\n",
    "- In the class constructor, define the network’s layers as class attributes using pre-built layers from `torch.nn`.\n",
    "- Use the network’s layer attributes as well as operations from the `nn.functional` API to define the network’s forward pass."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a trivial neural network (zero layers)\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init()\n",
    "        self.layer = None\n",
    "\n",
    "    def forward(self, t):\n",
    "        t = self.layer(t)\n",
    "        return t\n"
   ]
  },
  {
   "source": [
    "Let’s replace this now with some real layers that come pre-built for us from PyTorch's `nn` library. We’re building a CNN, so the two types of layers we'll use are linear layers and convolutional layers."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "\n",
    "    def forward(self, t):\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Network(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "  (out): Linear(in_features=60, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "network = Network()\n",
    "network"
   ]
  },
  {
   "source": [
    "We used the abbreviation `fc` in `fc1` and `fc2` because linear layers are also called fully connected layers. They also have a third name that we may hear sometimes called dense. So linear, dense, and fully connected are all ways to refer to the same type of layer. PyTorch uses the word linear, hence the `nn.Linear` class name.\n",
    "\n",
    "We used the name `out` for the last linear layer because the last layer in the network is the output layer.\n",
    "\n",
    "The above neural net has three hyperparameters that need to be manually specified:\n",
    "* `kernel_size.`  size of each convolutional filter\n",
    "* `out_channels` number of filters in the convolutional layer \n",
    "* `out_features` size of output tensor, i.e. the number of neurons in the dense layer\n",
    "\n",
    "Having `out_features=10` on the final output layer is a data dependent hyperparameter, i.e. fixed due to the nature of the problem."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### CNN Layer Parameters\n",
    "\n",
    "### Parameters vs Arguments\n",
    "\n",
    "Well parameters are used in function definitions as place-holders while arguments are the actual values that are passed to the function. The parameters can be thought of as local variables that live inside a function.\n",
    "\n",
    "In our network's case, the names are the parameters and the values that we have specified are the arguments."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Two Types of Parameters\n",
    "\n",
    "To better understand the argument values for these parameters, let's consider two categories or types of parameters that we used when constructing our layers.\n",
    "\n",
    "- Hyperparameters\n",
    "- Data dependent hyperparameters\n",
    "\n",
    "A lot of terms in deep learning are used loosely, and the word parameter is one of them. Try not to let it through you off. The main thing to remember about any type of parameter is that the parameter is a place-holder that will eventually hold or have a value.\n",
    "\n",
    "The goal of these particular categories is to help us remember how each parameter's value is decided.\n",
    "\n",
    "When we construct a layer, we pass values for each parameter to the layer’s constructor. With our convolutional layers have three parameters and the linear layers have two parameters.\n",
    "\n",
    "- Convolutional layers\n",
    "    - in_channels\n",
    "    - out_channels  - Sets the number of filters. One filter produces one output channel.\n",
    "    - kernel_size   - Sets the filter size. The words kernel and filter are interchangeable.\n",
    "\n",
    "- Linear layers\n",
    "    - in_features\n",
    "    - out_features - Sets the size of the output tensor.\n",
    "\n",
    "#### Hyperparameters\n",
    "In general, hyperparameters are parameters whose values are chosen manually and arbitrarily.\n",
    "\n",
    "As neural network programmers, we choose hyperparameter values mainly based on trial and error and increasingly by utilizing values that have proven to work well in the past. For building our CNN layers, these are the parameters we choose manually.\n",
    "\n",
    "- `kernel_size`\n",
    "- `out_channels`\n",
    "- `out_features`\n",
    "\n",
    "This means we simply choose the values for these parameters. In neural network programming, this is pretty common, and we usually test and tune these parameters to find values that work best.\n",
    "\n",
    "One pattern that shows up quite often is that we increase our out_channels as we add additional conv layers, and after we switch to linear layers we shrink our out_features as we filter down to our number of output classes.\n",
    "\n",
    "#### Data Dependent Hyperparameters\n",
    "Data dependent hyperparameters are parameters whose values are dependent on data. The first two data dependent hyperparameters that stick out are the `in_channels` of the first convolutional layer, and the `out_features` of the output layer.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "| Layer \t| Param name   \t| Param value \t| The param value is                                      \t|\n",
    "|-------\t|--------------\t|-------------\t|---------------------------------------------------------\t|\n",
    "| conv1 \t| in_channels  \t| 1           \t| the number of color channels in the input image.        \t|\n",
    "| conv1 \t| kernel_size  \t| 5           \t| a hyperparameter.                                       \t|\n",
    "| conv1 \t| out_channels \t| 6           \t| a hyperparameter.                                       \t|\n",
    "| conv2 \t| in_channels  \t| 6           \t| the number of out_channels in previous layer.           \t|\n",
    "| conv2 \t| kernel_size  \t| 5           \t| a hyperparameter.                                       \t|\n",
    "| conv2 \t| out_channels \t| 12          \t| a hyperparameter (higher than previous conv layer).     \t|\n",
    "| fc1   \t| in_features  \t| 12\\*4\\*4      \t| the length of the flattened output from previous layer. \t|\n",
    "| fc1   \t| out_features \t| 120         \t| a hyperparameter.                                       \t|\n",
    "| fc2   \t| in_features  \t| 120         \t| the number of out_features of previous layer.           \t|\n",
    "| fc2   \t| out_features \t| 60          \t| a hyperparameter (lower than previous linear layer).    \t|\n",
    "| out   \t| in_features  \t| 60          \t| the number of out_channels in previous layer.           \t|\n",
    "| out   \t| out_features \t| 10          \t| the number of prediction classes.                       \t|"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Learnable Parameters\n",
    "\n",
    "Learnable parameters are parameters whose values are learned during the training process.\n",
    "\n",
    "With learnable parameters, we typically start out with a set of arbitrary values, and these values then get updated in an iterative fashion as the network learns.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Network(\n  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=192, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=60, bias=True)\n  (out): Linear(in_features=60, out_features=10, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "print(network)"
   ]
  },
  {
   "source": [
    "The `print()` function prints to the console a string representation of our network. With a sharp eye, we can notice that the printed output here is detailing our network’s architecture listing out our network’s layers, and showing the values that were passed to the layer constructors.\n",
    "\n",
    "For this reason, in object oriented programming, we usually want to provide a string representation of our object inside our classes so that we get useful information when the object is printed. This string representation comes from Python’s default base class called object.\n",
    "\n",
    "We can override Python’s default string representation using the `__repr__` function. This name is short for representation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "For the convolutional layers, the kernel_size argument is a Python tuple `(5,5)` even though we only passed the number `5` in the constructor.\n",
    "\n",
    "This is because our filters actually have a height and width, and when we pass a single number, the code inside the layer’s constructor assumes that we want a square filter.\n",
    "\n",
    "The **stride** is an additional parameter that we could have set, but we left it out. When the stride is not specified in the layer constructor the layer automatically sets it.\n",
    "\n",
    "The **stride** tells the conv layer how far the filter should slide after each operation in the overall convolution. This tuple says to slide by one unit when moving to the right and also by one unit when moving down."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Accessing the Network's layers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "network.conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "network.conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Linear(in_features=192, out_features=120, bias=True)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "network.fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Linear(in_features=120, out_features=60, bias=True)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "network.fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Linear(in_features=60, out_features=10, bias=True)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "network.out"
   ]
  },
  {
   "source": [
    "### Accessing the Layer Weights"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-8.7929e-02,  1.3955e-01, -9.3954e-02,  1.1499e-01,  1.6544e-01],\n",
       "          [-1.4125e-01,  3.9365e-02,  4.9459e-02, -5.6673e-02,  1.9431e-01],\n",
       "          [-4.6090e-02, -9.1536e-02,  2.6325e-02, -1.2399e-01, -7.2885e-02],\n",
       "          [-2.5194e-02, -1.0776e-01,  1.6361e-01,  1.8289e-01, -9.1989e-02],\n",
       "          [-1.8407e-01, -7.5259e-02,  1.2855e-01,  7.8714e-02,  9.9350e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 1.7569e-01,  1.9174e-01, -1.0502e-01, -4.6414e-02, -1.5498e-01],\n",
       "          [ 1.8295e-01, -7.0241e-02,  9.3809e-02,  1.2386e-01,  5.8582e-02],\n",
       "          [-1.7224e-01, -1.9304e-01,  8.7031e-02, -8.6348e-02,  1.8299e-02],\n",
       "          [-5.3858e-02, -2.1679e-02,  1.1365e-01,  6.0892e-02,  8.5255e-02],\n",
       "          [-1.6609e-01,  1.4400e-01, -4.6355e-02,  1.4427e-01, -1.1424e-02]]],\n",
       "\n",
       "\n",
       "        [[[-1.6910e-01,  2.6303e-02, -1.1944e-01,  1.0375e-01, -1.5363e-01],\n",
       "          [ 1.9425e-01, -1.2621e-04,  1.7791e-01,  7.1151e-02, -4.0848e-02],\n",
       "          [ 1.5346e-02,  3.0460e-02,  1.2368e-01,  3.7531e-02,  6.3887e-02],\n",
       "          [ 7.6040e-03,  1.9448e-01, -1.9904e-01, -1.8397e-01,  1.5478e-01],\n",
       "          [-1.8308e-01,  1.9172e-01, -1.5327e-01, -6.1842e-02,  9.9145e-03]]],\n",
       "\n",
       "\n",
       "        [[[-1.5299e-01, -3.1429e-03, -1.7183e-01, -1.7064e-01,  9.1137e-02],\n",
       "          [ 1.2760e-01, -5.9619e-02, -1.9562e-01, -1.5605e-01,  1.3253e-01],\n",
       "          [ 1.2761e-02,  1.9430e-01,  4.0612e-03, -7.0408e-02,  1.8634e-01],\n",
       "          [-3.5421e-02,  7.6657e-02, -1.6669e-01,  5.7110e-02,  9.6785e-02],\n",
       "          [ 1.7823e-01,  1.2145e-01,  4.3639e-02,  2.7724e-02, -1.7554e-01]]],\n",
       "\n",
       "\n",
       "        [[[-6.1090e-02, -1.4543e-01,  8.8457e-02,  5.9023e-02,  1.3263e-01],\n",
       "          [ 1.7464e-01,  1.1753e-01,  8.5004e-02, -1.4838e-01, -8.0576e-02],\n",
       "          [ 1.0965e-01,  4.8484e-02,  1.4957e-01, -1.5971e-01, -3.3046e-03],\n",
       "          [ 6.9327e-02,  7.5812e-02,  1.6667e-01, -1.0046e-01, -6.8020e-02],\n",
       "          [ 1.5417e-01, -1.1589e-01, -7.3488e-02,  1.0747e-01,  1.3890e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 6.9618e-02, -1.0430e-01, -7.4401e-02,  1.8432e-01,  8.1084e-02],\n",
       "          [ 1.1887e-01, -1.0053e-01,  1.4916e-01, -7.7017e-02, -1.2530e-01],\n",
       "          [ 1.2432e-01,  8.2857e-02, -2.9131e-02,  9.8397e-02, -1.6531e-01],\n",
       "          [ 1.6215e-01,  1.8355e-01, -4.3575e-02,  1.0910e-01,  1.0673e-01],\n",
       "          [ 1.7432e-01, -1.9331e-01, -4.0933e-02, -2.6984e-02, -9.9294e-02]]]],\n",
       "       requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "network.conv1.weight"
   ]
  },
  {
   "source": [
    "Then, we access the weight tensor object that lives inside the conv layer object, so all of these objects are chained or linked together.\n",
    "\n",
    "One thing to notice about the weight tensor output is that it says parameter containing at the top of the output. This is because this particular tensor is a special tensor because its values or scalar components are learnable parameters of our network.\n",
    "\n",
    "This means that the values inside this tensor, the ones we see above, are actually learned as the network is trained. As we train, these weight values are updated in such a way that the loss function is minimized.\n",
    "\n",
    "\n",
    "### Parameter Class\n",
    "To keep track of all the weight tensors inside the network. PyTorch has a special class called Parameter. The Parameter class extends the tensor class, and so the weight tensor inside every layer is an instance of this Parameter class. This is why we see the Parameter containing text at the top of the string representation output.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Weight Tensor Shape\n",
    "\n",
    "For the convolutional layers, the weight values live inside the filters, and in code, the filters are actually the weight tensors themselves.\n",
    "\n",
    "The convolution operation inside a layer is an operation between the input channels to the layer and the filter inside the layer. This means that what we really have is an operation between two tensors."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 5, 5])"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "network.conv1.weight.shape"
   ]
  },
  {
   "source": [
    "For the first conv layer, we have 1 color channel that should be convolved by 6 filters of size 5x5 to produce 6 output channels. This is how we interpret the values inside our layer constructor.\n",
    "\n",
    "Inside our layer though, we don’t explicitly have 6 weight tensors for each of the 6 filters. We actually represent all 6 filters using a single weight tensor whose shape reflects or accounts for the 6 filters.\n",
    "\n",
    "The shape of the weight tensor for the first convolutional layer shows us that we have a rank-4 weight tensor. The first axis has a length of 6, and this accounts for the 6 filters."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([12, 6, 5, 5])"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "network.conv2.weight.shape"
   ]
  },
  {
   "source": [
    "Think of this value of 6 here as giving each of the filters some depth. Instead of having a filter that convolves all of the channels iteratively, our filter has a depth that matches the number of channels.\n",
    "\n",
    "The two main takeaways about these convolutional layers is that our filters are represented using a single tensor and that each filter inside the tensor also has a depth that accounts for the input channels that are being convolved.\n",
    "\n",
    "- All filters are represented using a single tensor.\n",
    "- Filters have depth that accounts for the input channels.\n",
    "\n",
    "**Our tensors are rank-4 tensors.** \n",
    "\n",
    "The first axis represents the number of filters. The second axis represents the depth of each filter which corresponds to the number of input channels being convolved.\n",
    "\n",
    "The last two axes represent the height and width of each filter. We can pull out any single filter by indexing into the weight tensor’s first axis.\n",
    "\n",
    "__(Number of filters, Depth, Height, Width)__\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Network(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "  (out): Linear(in_features=60, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "network"
   ]
  },
  {
   "source": [
    "With linear layers or fully connected layers, we have flattened rank-1 tensors as input and as output. The way we transform the in_features to the out_features in a linear layer is by using a rank-2 tensor that is commonly called a weight matrix.\n",
    "\n",
    "This is due to the fact that the weight tensor is of rank-2 with height and width axes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([120, 192])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "network.fc1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([60, 120])"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "network.fc2.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([10, 60])"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "network.out.weight.shape"
   ]
  },
  {
   "source": [
    "### A general example of Matrix Multiplication\n",
    "\n",
    "Here, we have the `in_features` and the `weight_matrix` as tensors, and we’re using the tensor method called matmul() to perform the operation. The name `matmul()` as we now know is short for matrix multiplication.\n",
    "\n",
    "In general, the weight matrix defines a linear function that maps a 1-dimensional tensor with four elements to a 1-dimensional tensor that has three elements. **We can think of this function as a mapping from 4-dimensional Euclidean space to 3-dimensional Euclidean space.**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_feature = torch.tensor([1, 2, 3, 4], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_matrix = torch.tensor([\n",
    "    [1, 2, 3, 4],\n",
    "    [2, 3 ,4, 5],\n",
    "    [3, 4, 5, 6]\n",
    "], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([30., 40., 50.])"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "weight_matrix.matmul(in_feature)"
   ]
  },
  {
   "source": [
    "### Accessing the Network Parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([6, 1, 5, 5])\ntorch.Size([6])\ntorch.Size([12, 6, 5, 5])\ntorch.Size([12])\ntorch.Size([120, 192])\ntorch.Size([120])\ntorch.Size([60, 120])\ntorch.Size([60])\ntorch.Size([10, 60])\ntorch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for param in network.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "conv1.weight \t\t torch.Size([6, 1, 5, 5])\nconv1.bias \t\t torch.Size([6])\nconv2.weight \t\t torch.Size([12, 6, 5, 5])\nconv2.bias \t\t torch.Size([12])\nfc1.weight \t\t torch.Size([120, 192])\nfc1.bias \t\t torch.Size([120])\nfc2.weight \t\t torch.Size([60, 120])\nfc2.bias \t\t torch.Size([60])\nout.weight \t\t torch.Size([10, 60])\nout.bias \t\t torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name, param in network.named_parameters():\n",
    "    print(name, '\\t\\t', param.shape)"
   ]
  },
  {
   "source": [
    "### PyTorch Linear Layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = nn.Linear(in_features=4, out_features=3)"
   ]
  },
  {
   "source": [
    "#### Callable Python Objects\n",
    "\n",
    "PyTorch creates a weight matrix and initializes it with random values. This means that the linear functions from the two examples are different, so we are using different function to produce these outputs."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([-1.5193, -1.6051, -0.4588], grad_fn=<AddBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "fc(in_feature)"
   ]
  },
  {
   "source": [
    "Let's explicitly set the weight matrix of the linear layer to be the same as the one we used in our other example.\n",
    "\n",
    "PyTorch module weights need to be parameters. This is why we wrap the weight matrix tensor inside a parameter class instance. Let's see now how this layer transforms the input using the new weight matrix. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc.weight = nn.Parameter(weight_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([29.9192, 40.2606, 50.4969], grad_fn=<AddBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "fc(in_feature)"
   ]
  },
  {
   "source": [
    "This time we are much closer to the 30, 40, and 50 values. However, we're exact. \n",
    "\n",
    "Why is this? \n",
    "\n",
    "We'll, this is not exact because the linear layer is adding a bias tensor to the output. Watch what happens when we turn the bias off. We do this by passing a False flag to the constructor."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = nn.Linear(in_features=4, out_features=3, bias=False)\n",
    "fc.weight = nn.Parameter(weight_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([30., 40., 50.], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "fc(in_feature)"
   ]
  },
  {
   "source": [
    "## Callable Layers and Neural Networks\n",
    "\n",
    "We pointed out before how it was kind of strange that we called the layer object instance as if it were a function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "What makes this possible is that PyTorch module classes implement another special Python function called `__call__()`. If a class implements the `__call__()` method, the special call method will be invoked anytime the object instance is called.\n",
    "\n",
    "This fact is an important PyTorch concept because of the way the `__call__()` method interacts with the `forward()` method for our layers and networks.\n",
    "\n",
    "Instead of calling the `forward()` method directly, we call the object instance. After the object instance is called, the `__call__()` method is invoked under the hood, and the `__call__()` in turn invokes the `forward()` method. This applies to all PyTorch neural network modules, namely, networks and layers."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([-2.8435, -0.2904,  0.0378], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "fc = nn.Linear(in_features=4, out_features=3)\n",
    "\n",
    "t = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "\n",
    "output = fc(t)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "source": [
    "The extra code that PyTorch runs inside the `__call__()` method is why we never invoke the `forward()` method directly. If we did, the additional PyTorch code would not be executed. As a result, any time we want to invoke our `forward()` method, we call the object instance. This applies to both layers, and networks because they are both PyTorch neural network modules."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Forward Propagation\n",
    "## Implementing the `forward()` method\n",
    "\n",
    "The `forward()` method is the actual network transformation. The forward method is the mapping that maps an input tensor to a prediction output tensor. Let's see how this is done."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "\n",
    "    def forward(self, t):\n",
    "        # (1) input layer\n",
    "        t = t\n",
    "\n",
    "        # (2) hidden convolution layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # (3) hiden convolution layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        return t"
   ]
  },
  {
   "source": [
    "As we can see here, our input tensor is transformed as we move through the convolutional layers. \n",
    "- The first convolutional layer has a convolutional operation, followed by a relu activation operation whose output is then passed to a max pooling operation with kernel_size=2 and stride=2.\n",
    "- The output tensor t of the first convolutional layer is then passed to the next convolutional layer, which is identical except for the fact that we call self.conv2() instead of self.conv1().\n",
    "\n",
    "Each of these layers is comprised of a collection of weights (data) and a collection operations (code). \n",
    "The weights are encapsulated inside the `nn.Conv2d()` class instance. \n",
    "\n",
    "The `relu()` and the `max_pool2d()` calls are just pure operations. Neither of these have weights, and this is why we call them directly from the `nn.functional` API.\n",
    "\n",
    "Sometimes we may see pooling operations referred to as ***pooling*** layers. Sometimes we may even hear activation operations called ***activation*** layers."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The 4 * 4 is actually the height and width of each of the 12 output channels.\n",
    "\n",
    "We started with a 1 x 28 x 28 input tensor. This gives a single color channel, 28 x 28 image, and by the time our tensor arrives at the first linear layer, the dimensions have changed.\n",
    "\n",
    "The height and width dimensions have been reduced from 28 x 28 to 4 x 4 by the convolution and pooling operations.\n",
    "\n",
    "Before we pass our input to the first hidden linear layer, we must `reshape()` or flatten our tensor. This will be the case any time we are passing output from a convolutional layer as input to a linear layer.\n",
    "\n",
    "Since the fourth layer is the first linear layer, we will include our reshaping operation as a part of the fourth layer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "torch.set_printoptions(linewidth=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root = './data/FashionMNIST',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "\n",
    "    def forward(self, t):\n",
    "        # (1) input layer\n",
    "        t = t\n",
    "\n",
    "        # (2) hidden convolution layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # (3) hiden convolution layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        # (4) hidden linear layer\n",
    "        t = t.reshape(-1, 12 * 4 * 4)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        # (5) hidden linear layer\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        # (6) output layer\n",
    "        t = self.out(t)\n",
    "        #t = F.softmax(t, dim=1)\n",
    "\n",
    "        return t"
   ]
  },
  {
   "source": [
    "Inside the network we usually use `relu()` as our non-linear activation function, but for the output layer, whenever we have a single category that we are trying to predict, we use `softmax()`. \n",
    "\n",
    "**The softmax function returns a positive probability for each of the prediction classes, and the probabilities sum to 1.**\n",
    "\n",
    "However, in our case, we won't use `softmax()` because the loss function that we'll use, `F.cross_entropy()`, implicitly performs the `softmax()` operation on its input, so we'll just return the result of the last linear transformation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f9f27316690>"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "image, label = sample\n",
    "image.shape"
   ]
  },
  {
   "source": [
    "### Create a Batch\n",
    "\n",
    "The image tensor’s shape indicates that we have a single channel image that is 28 in height and 28 in width. Cool, this is what we expect.\n",
    "\n",
    "Now, there's a second step we must preform before simply passing this tensor to our network. When we pass a tensor to our network, the network is expecting a batch, so even if we want to pass a single image, we still need a batch.\n",
    "\n",
    "This is no problem. We can create a batch that contains a single image. All of this will be packaged into a single four dimensional tensor that reflects the following dimensions.\n",
    "\n",
    "This requirement of the network arises from the fact that the `forward()` method's in the `nn.Conv2d` convolutional layer classes expect their tensors to have 4 dimensions. This is pretty standard as most neural network implementations deal with batches of input samples rather than single samples."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "image.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = network(image.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 0.0585, -0.0700,  0.0938,  0.0281, -0.0105,  0.1163, -0.0674,  0.1468,  0.0291, -0.0316]])"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "source": [
    "The shape of the prediction tensor is `1 x 10`. This tells us that the first axis has a length of one while the second axis has a length of ten. The interpretation of this is that we have one image in our batch and ten prediction classes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([7])"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "pred.argmax(dim=1)"
   ]
  },
  {
   "source": [
    "For each input in the batch, and for each prediction class, we have a prediction value. If we wanted these values to be probabilities, we could just the `softmax()` function from the `nn.functional` package."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.1027, 0.0903, 0.1064, 0.0996, 0.0959, 0.1088, 0.0905, 0.1122, 0.0997, 0.0939]])"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "F.softmax(pred, dim=1)"
   ]
  },
  {
   "source": [
    "There are a couple of important things we need to point out about these results. Most of the probabilities came in close to `10%`, and this makes sense because our network is guessing and we have ten prediction classes coming from a `balanced dataset`.\n",
    "\n",
    "Another implication of the randomly generated weights is that each time we create a new instance of our network, the weights within the network will be different. This means that the predictions we get will be different if we create different networks. Keep this in mind. Your predictions will be different from what we see here."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Sending data in batches"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.3.1\n0.4.2\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 28, 28])"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "source": [
    "Last time, when we pulled a single image from our training set, we had to `unsqueeze()` the tensor to add another dimension that would effectively transform the singleton image into a batch with a size of one. Now that we are working with the data loader, we are dealing with batches by default, so there is no further processing needed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = network(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "source": [
    "The prediction tensor has a shape of `10 by 10`, which gives us two axes that each have a length of ten. This reflects the fact that we have ten images and for each of these ten images we have ten prediction classes.\n",
    "`(batch size, number of prediction classes)`\n",
    "\n",
    "The elements of the first dimension are arrays of length ten. Each of these array elements contain the ten predictions for each category for the corresponding image.\n",
    "\n",
    "The elements of the second dimension are numbers. Each number is the assigned value of the specific output class. The output classes are encoded by the indexes, so each index represents a specific output class."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 0.0585, -0.0700,  0.0938,  0.0281, -0.0105,  0.1163, -0.0674,  0.1468,  0.0291, -0.0316],\n",
       "        [ 0.0598, -0.0740,  0.0904,  0.0377, -0.0031,  0.1237, -0.0661,  0.1564,  0.0247, -0.0305],\n",
       "        [ 0.0596, -0.0657,  0.0894,  0.0210, -0.0019,  0.1215, -0.0722,  0.1487,  0.0284, -0.0404],\n",
       "        [ 0.0579, -0.0662,  0.0900,  0.0240, -0.0042,  0.1205, -0.0739,  0.1525,  0.0294, -0.0380],\n",
       "        [ 0.0547, -0.0669,  0.0908,  0.0265, -0.0071,  0.1156, -0.0662,  0.1493,  0.0265, -0.0303],\n",
       "        [ 0.0574, -0.0690,  0.0954,  0.0363, -0.0056,  0.1224, -0.0676,  0.1516,  0.0264, -0.0295],\n",
       "        [ 0.0674, -0.0768,  0.0996,  0.0245, -0.0041,  0.1313, -0.0768,  0.1605,  0.0312, -0.0419],\n",
       "        [ 0.0619, -0.0714,  0.0969,  0.0326, -0.0072,  0.1201, -0.0704,  0.1503,  0.0232, -0.0294],\n",
       "        [ 0.0532, -0.0634,  0.0898,  0.0267, -0.0115,  0.1146, -0.0650,  0.1440,  0.0232, -0.0306],\n",
       "        [ 0.0524, -0.0596,  0.0984,  0.0273, -0.0117,  0.1121, -0.0655,  0.1508,  0.0262, -0.0288]])"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "source": [
    "The result from the `argmax()` function is a tensor of ten prediction categories. \n",
    "\n",
    "Each number is the index where the highest value occurred. We have ten numbers because there were ten images. Once we have this tensor of indices of highest values, we can compare it against the label tensor."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7])"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "preds.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5])"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False,  True, False, False, False])"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "preds.argmax(dim=1).eq(labels)"
   ]
  },
  {
   "source": [
    "Finally, if we call the `sum()` function on this result, we can reduce the output into a single number of correct predictions inside this scalar valued tensor."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "get_num_correct(preds, labels)"
   ]
  },
  {
   "source": [
    "## CNN Output Size Formula - Tensor Transformations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- Suppose we have an `n_h x n_w` input.\n",
    "- Suppose we have an `f_h x f_w` filter.\n",
    "- Suppose we have a padding of `p` and a stride of `s`.\n",
    "\n",
    "The height of the output size `O_h` is given by the formula : \n",
    "\n",
    "    O_h = (n_h - f_h + 2p)/s + 1\n",
    "\n",
    "The width of the output size `O_w` is given by the formula : \n",
    "\n",
    "    O_w = (n_w - f_w + 2p)/s + 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "| Operation             \t| Output Shape               \t|\n",
    "|-----------------------\t|----------------------------\t|\n",
    "| Identity function     \t| torch.Size([1, 1, 28, 28]) \t|\n",
    "| Convolution (5 x 5)   \t| torch.Size([1, 6, 24, 24]) \t|\n",
    "| Max pooling (2 x 2)   \t| torch.Size([1, 6, 12, 12]) \t|\n",
    "| Convolution (5 x 5)   \t| torch.Size([1, 12, 8, 8])  \t|\n",
    "| Max pooling (2 x 2)   \t| torch.Size([1, 12, 4, 4])  \t|\n",
    "| Flatten (reshape)     \t| torch.Size([1, 192])       \t|\n",
    "| Linear transformation \t| torch.Size([1, 120])       \t|\n",
    "| Linear transformation \t| torch.Size([1, 60])        \t|\n",
    "| Linear transformation \t| torch.Size([1, 10])        \t|"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}