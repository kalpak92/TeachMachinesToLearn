{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recitation - 6\n",
    "___\n",
    "\n",
    "* Custom Dataset & DataLoader\n",
    "* Torchvision ImageFolder Dataset\n",
    "* Residual Block\n",
    "* CNN model with Residual Block\n",
    "* Loss Functions (Center Loss and Triplet Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision   \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom DataSet with DataLoader\n",
    "___\n",
    "We have used a subset of the data given for the Face Classification and Verification problem in Part 2 of the homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, file_list, target_list):\n",
    "        self.file_list = file_list\n",
    "        self.target_list = target_list\n",
    "        self.n_class = len(list(set(target_list)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.file_list[index])\n",
    "        img = torchvision.transforms.ToTensor()(img)\n",
    "        label = self.target_list[index]\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse the given directory to accumulate all the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(datadir):\n",
    "    img_list = []\n",
    "    ID_list = []\n",
    "    for root, directories, filenames in os.walk(datadir):  #root: median/1\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.jpg'):\n",
    "                filei = os.path.join(root, filename)\n",
    "                img_list.append(filei)\n",
    "                ID_list.append(root.split('/')[-1])\n",
    "\n",
    "    # construct a dictionary, where key and value correspond to ID and target\n",
    "    uniqueID_list = list(set(ID_list))\n",
    "    class_n = len(uniqueID_list)\n",
    "    target_dict = dict(zip(uniqueID_list, range(class_n)))\n",
    "    label_list = [target_dict[ID_key] for ID_key in ID_list]\n",
    "\n",
    "    print('{}\\t\\t{}\\n{}\\t\\t{}'.format('#Images', '#Labels', len(img_list), len(set(label_list))))\n",
    "    return img_list, label_list, class_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Images\t\t#Labels\n",
      "1889\t\t5\n"
     ]
    }
   ],
   "source": [
    "img_list, label_list, class_n = parse_data('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medium/5/0084_07.jpg\n"
     ]
    }
   ],
   "source": [
    "print(img_list[1888])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = ImageDataset(img_list, label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_item, train_data_label = trainset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data item shape: torch.Size([3, 32, 32])\t data item label: 2\n"
     ]
    }
   ],
   "source": [
    "print('data item shape: {}\\t data item label: {}'.format(train_data_item.shape, train_data_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(trainset, batch_size=10, shuffle=True, num_workers=1, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchvision DataSet and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageFolder_dataset = torchvision.datasets.ImageFolder(root='medium/', \n",
    "                                                       transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageFolder_dataloader = DataLoader(imageFolder_dataset, batch_size=10, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1889, 5)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imageFolder_dataset.__len__(), len(imageFolder_dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Block\n",
    "\n",
    "Resnet: https://arxiv.org/pdf/1512.03385.pdf\n",
    "\n",
    "Here is a basic usage of shortcut in Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, channel_size, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channel_size, channel_size, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channel_size)\n",
    "        self.shortcut = nn.Conv2d(channel_size, channel_size, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn1(self.conv1(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model with Residual Block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, num_feats, hidden_sizes, num_classes, feat_dim=10):\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.hidden_sizes = [num_feats] + hidden_sizes + [num_classes]\n",
    "        \n",
    "        self.layers = []\n",
    "        for idx, channel_size in enumerate(hidden_sizes):\n",
    "            self.layers.append(nn.Conv2d(in_channels=self.hidden_sizes[idx], \n",
    "                                         out_channels=self.hidden_sizes[idx+1], \n",
    "                                         kernel_size=3, stride=2, bias=False))\n",
    "            self.layers.append(nn.ReLU(inplace=True))\n",
    "            self.layers.append(BasicBlock(channel_size = channel_size))\n",
    "            \n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "        self.linear_label = nn.Linear(self.hidden_sizes[-2], self.hidden_sizes[-1], bias=False)\n",
    "        \n",
    "        # For creating the embedding to be passed into the Center Loss criterion\n",
    "        self.linear_closs = nn.Linear(self.hidden_sizes[-2], feat_dim, bias=False)\n",
    "        self.relu_closs = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x, evalMode=False):\n",
    "        output = x\n",
    "        output = self.layers(output)\n",
    "            \n",
    "        output = F.avg_pool2d(output, [output.size(2), output.size(3)], stride=1)\n",
    "        output = output.reshape(output.shape[0], output.shape[1])\n",
    "        \n",
    "        label_output = self.linear_label(output)\n",
    "        label_output = label_output/torch.norm(self.linear_label.weight, dim=1)\n",
    "        \n",
    "        # Create the feature embedding for the Center Loss\n",
    "        closs_output = self.linear_closs(output)\n",
    "        closs_output = self.relu_closs(closs_output)\n",
    "\n",
    "        return closs_output, label_output\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d or type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, test_loader, task='Classification'):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(numEpochs):\n",
    "        avg_loss = 0.0\n",
    "        for batch_num, (feats, labels) in enumerate(data_loader):\n",
    "            feats, labels = feats.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(feats)[1]\n",
    "\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            if batch_num % 50 == 49:\n",
    "                print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/50))\n",
    "                avg_loss = 0.0    \n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            del feats\n",
    "            del labels\n",
    "            del loss\n",
    "        \n",
    "        if task == 'Classification':\n",
    "            val_loss, val_acc = test_classify(model, test_loader)\n",
    "            train_loss, train_acc = test_classify(model, data_loader)\n",
    "            print('Train Loss: {:.4f}\\tTrain Accuracy: {:.4f}\\tVal Loss: {:.4f}\\tVal Accuracy: {:.4f}'.\n",
    "                  format(train_loss, train_acc, val_loss, val_acc))\n",
    "        else:\n",
    "            test_verify(model, test_loader)\n",
    "\n",
    "\n",
    "def test_classify(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = []\n",
    "    accuracy = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_num, (feats, labels) in enumerate(test_loader):\n",
    "        feats, labels = feats.to(device), labels.to(device)\n",
    "        outputs = model(feats)[1]\n",
    "        \n",
    "        _, pred_labels = torch.max(F.softmax(outputs, dim=1), 1)\n",
    "        pred_labels = pred_labels.view(-1)\n",
    "        \n",
    "        loss = criterion(outputs, labels.long())\n",
    "        \n",
    "        accuracy += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "        total += len(labels)\n",
    "        test_loss.extend([loss.item()]*feats.size()[0])\n",
    "        del feats\n",
    "        del labels\n",
    "\n",
    "    model.train()\n",
    "    return np.mean(test_loss), accuracy/total\n",
    "\n",
    "\n",
    "def test_verify(model, test_loader):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset, DataLoader and Constant Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.ImageFolder(root='medium/', \n",
    "                                                 transform=torchvision.transforms.ToTensor())\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, \n",
    "                                               shuffle=True, num_workers=8)\n",
    "\n",
    "dev_dataset = torchvision.datasets.ImageFolder(root='medium_dev/', \n",
    "                                               transform=torchvision.transforms.ToTensor())\n",
    "dev_dataloader = torch.utils.data.DataLoader(dev_dataset, batch_size=10, \n",
    "                                             shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "numEpochs = 4\n",
    "num_feats = 3\n",
    "\n",
    "learningRate = 1e-2\n",
    "weightDecay = 5e-5\n",
    "\n",
    "hidden_sizes = [32, 64]\n",
    "num_classes = len(train_dataset.classes)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network(num_feats, hidden_sizes, num_classes)\n",
    "network.apply(init_weights)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=learningRate, weight_decay=weightDecay, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tBatch: 50\tAvg-Loss: 1.5523\n",
      "Epoch: 1\tBatch: 100\tAvg-Loss: 1.3623\n",
      "Epoch: 1\tBatch: 150\tAvg-Loss: 1.3098\n",
      "Train Loss: 1.4772\tTrain Accuracy: 0.3954\tVal Loss: 1.4213\tVal Accuracy: 0.5000\n",
      "Epoch: 2\tBatch: 50\tAvg-Loss: 1.2030\n",
      "Epoch: 2\tBatch: 100\tAvg-Loss: 1.0246\n",
      "Epoch: 2\tBatch: 150\tAvg-Loss: 1.0020\n",
      "Train Loss: 1.5984\tTrain Accuracy: 0.1863\tVal Loss: 1.6418\tVal Accuracy: 0.2000\n",
      "Epoch: 3\tBatch: 50\tAvg-Loss: 0.9659\n",
      "Epoch: 3\tBatch: 100\tAvg-Loss: 0.9349\n",
      "Epoch: 3\tBatch: 150\tAvg-Loss: 1.0144\n",
      "Train Loss: 1.5400\tTrain Accuracy: 0.2483\tVal Loss: 1.6190\tVal Accuracy: 0.2000\n",
      "Epoch: 4\tBatch: 50\tAvg-Loss: 0.8472\n",
      "Epoch: 4\tBatch: 100\tAvg-Loss: 0.8466\n",
      "Epoch: 4\tBatch: 150\tAvg-Loss: 0.7887\n",
      "Train Loss: 1.7215\tTrain Accuracy: 0.2546\tVal Loss: 1.7385\tVal Accuracy: 0.3000\n"
     ]
    }
   ],
   "source": [
    "network.train()\n",
    "network.to(device)\n",
    "train(network, train_dataloader, dev_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Center Loss\n",
    "___\n",
    "The following piece of code for Center Loss has been pulled and modified based on the code from the GitHub Repo: https://github.com/KaiyangZhou/pytorch-center-loss\n",
    "    \n",
    "<b>Reference:</b>\n",
    "<i>Wen et al. A Discriminative Feature Learning Approach for Deep Face Recognition. ECCV 2016.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenterLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        feat_dim (int): feature dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, feat_dim, device=torch.device('cpu')):\n",
    "        super(CenterLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.feat_dim = feat_dim\n",
    "        self.device = device\n",
    "        \n",
    "        self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).to(self.device))\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: feature matrix with shape (batch_size, feat_dim).\n",
    "            labels: ground truth labels with shape (batch_size).\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\n",
    "                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\n",
    "        distmat.addmm_(1, -2, x, self.centers.t())\n",
    "\n",
    "        classes = torch.arange(self.num_classes).long().to(self.device)\n",
    "        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\n",
    "        mask = labels.eq(classes.expand(batch_size, self.num_classes))\n",
    "\n",
    "        dist = []\n",
    "        for i in range(batch_size):\n",
    "            value = distmat[i][mask[i]]\n",
    "            value = value.clamp(min=1e-12, max=1e+12) # for numerical stability\n",
    "            dist.append(value)\n",
    "        dist = torch.cat(dist)\n",
    "        loss = dist.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_closs(model, data_loader, test_loader, task='Classification'):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(numEpochs):\n",
    "        avg_loss = 0.0\n",
    "        for batch_num, (feats, labels) in enumerate(data_loader):\n",
    "            feats, labels = feats.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer_label.zero_grad()\n",
    "            optimizer_closs.zero_grad()\n",
    "            \n",
    "            feature, outputs = model(feats)\n",
    "\n",
    "            l_loss = criterion_label(outputs, labels.long())\n",
    "            c_loss = criterion_closs(feature, labels.long())\n",
    "            loss = l_loss + closs_weight * c_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer_label.step()\n",
    "            # by doing so, weight_cent would not impact on the learning of centers\n",
    "            for param in criterion_closs.parameters():\n",
    "                param.grad.data *= (1. / closs_weight)\n",
    "            optimizer_closs.step()\n",
    "            \n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            if batch_num % 50 == 49:\n",
    "                print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/50))\n",
    "                avg_loss = 0.0    \n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            del feats\n",
    "            del labels\n",
    "            del loss\n",
    "        \n",
    "        if task == 'Classification':\n",
    "            val_loss, val_acc = test_classify_closs(model, test_loader)\n",
    "            train_loss, train_acc = test_classify_closs(model, data_loader)\n",
    "            print('Train Loss: {:.4f}\\tTrain Accuracy: {:.4f}\\tVal Loss: {:.4f}\\tVal Accuracy: {:.4f}'.\n",
    "                  format(train_loss, train_acc, val_loss, val_acc))\n",
    "        else:\n",
    "            test_verify(model, test_loader)\n",
    "\n",
    "\n",
    "def test_classify_closs(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = []\n",
    "    accuracy = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_num, (feats, labels) in enumerate(test_loader):\n",
    "        feats, labels = feats.to(device), labels.to(device)\n",
    "        feature, outputs = model(feats)\n",
    "        \n",
    "        _, pred_labels = torch.max(F.softmax(outputs, dim=1), 1)\n",
    "        pred_labels = pred_labels.view(-1)\n",
    "        \n",
    "        l_loss = criterion_label(outputs, labels.long())\n",
    "        c_loss = criterion_closs(feature, labels.long())\n",
    "        loss = l_loss + closs_weight * c_loss\n",
    "        \n",
    "        accuracy += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "        total += len(labels)\n",
    "        test_loss.extend([loss.item()]*feats.size()[0])\n",
    "        del feats\n",
    "        del labels\n",
    "\n",
    "    model.train()\n",
    "    return np.mean(test_loss), accuracy/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "closs_weight = 1\n",
    "lr_cent = 0.5\n",
    "feat_dim = 10\n",
    "\n",
    "network = Network(num_feats, hidden_sizes, num_classes, feat_dim)\n",
    "network.apply(init_weights)\n",
    "\n",
    "criterion_label = nn.CrossEntropyLoss()\n",
    "criterion_closs = CenterLoss(num_classes, feat_dim, device)\n",
    "optimizer_label = torch.optim.SGD(network.parameters(), lr=learningRate, weight_decay=weightDecay, momentum=0.9)\n",
    "                 optimizer_closs = torch.optim.SGD(criterion_closs.parameters(), lr=lr_cent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tBatch: 50\tAvg-Loss: 2.0687\n",
      "Epoch: 1\tBatch: 100\tAvg-Loss: 1.4780\n",
      "Epoch: 1\tBatch: 150\tAvg-Loss: 1.3875\n",
      "Train Loss: 1.5936\tTrain Accuracy: 0.4246\tVal Loss: 1.6142\tVal Accuracy: 0.5000\n",
      "Epoch: 2\tBatch: 50\tAvg-Loss: 1.2771\n",
      "Epoch: 2\tBatch: 100\tAvg-Loss: 1.2463\n",
      "Epoch: 2\tBatch: 150\tAvg-Loss: 1.1943\n",
      "Train Loss: 1.6928\tTrain Accuracy: 0.3033\tVal Loss: 1.6583\tVal Accuracy: 0.3000\n",
      "Epoch: 3\tBatch: 50\tAvg-Loss: 1.1217\n",
      "Epoch: 3\tBatch: 100\tAvg-Loss: 1.0829\n",
      "Epoch: 3\tBatch: 150\tAvg-Loss: 1.0540\n",
      "Train Loss: 1.4977\tTrain Accuracy: 0.4479\tVal Loss: 1.4433\tVal Accuracy: 0.5000\n",
      "Epoch: 4\tBatch: 50\tAvg-Loss: 0.9645\n",
      "Epoch: 4\tBatch: 100\tAvg-Loss: 0.9230\n",
      "Epoch: 4\tBatch: 150\tAvg-Loss: 0.9544\n",
      "Train Loss: 1.4057\tTrain Accuracy: 0.4854\tVal Loss: 1.3658\tVal Accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "network.train()\n",
    "network.to(device)\n",
    "train_closs(network, train_dataloader, dev_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet Loss\n",
    "___\n",
    "You can make a dataloader that returns a tuple of three images. Two being from the same class and one from a different class. You can then use triplet loss to seperate out the different class pair distance and decrease same class pair distance.\n",
    "\n",
    "More on this link: https://github.com/adambielski/siamese-triplet/blob/master/losses.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3 2\n",
      "Loss=1.18\n"
     ]
    }
   ],
   "source": [
    "triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "face_img1, label_img1 = trainset.__getitem__(0)\n",
    "face_img2, label_img2 = trainset.__getitem__(1)\n",
    "face_img3, label_img3 = trainset.__getitem__(-1)\n",
    "\n",
    "print(label_img1, label_img2, label_img3)\n",
    "## face_img1 and face_img2 are from the same class and face_img3 is from a different class.\n",
    "loss = triplet_loss(face_img1, face_img2, face_img3)\n",
    "print (\"Loss={:0.2f}\".format(loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}